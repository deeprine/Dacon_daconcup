{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "from datetime import date, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.losses import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82109\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "info_compet = pd.read_csv(os.path.join('new_competition_info.csv'), encoding = 'euc-kr')\n",
    "info_login = pd.read_csv(os.path.join('new_login_info.csv'), encoding = 'euc-kr')\n",
    "info_user = pd.read_csv(os.path.join('new_user_info.csv'), encoding='euc-kr')\n",
    "info_sub = pd.read_csv(os.path.join('new_submission_info.csv'), encoding= 'euc-kr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipline(info_sub, info_compet, info_user, train):\n",
    "    \n",
    "    #info sub -----------------------------------------------------------------\n",
    "\n",
    "    info_sub['c_time'] = pd.to_datetime(info_sub['c_time'])\n",
    "    info_sub['date'] = info_sub.c_time.dt.date\n",
    "    info_sub['Weekday'] = info_sub['c_time'].dt.weekday\n",
    "    info_sub['YearMonth'] = info_sub['c_time'].dt.to_period('M')\n",
    "\n",
    "    day_group = info_sub.groupby(['YearMonth','date'])\n",
    "\n",
    "    id_col = ['sub_id']\n",
    "\n",
    "    info_group_datas = pd.concat([\n",
    "        day_group[id_col].count().rename(columns = lambda s : 'counts_' + s)\n",
    "    ], axis = 1).reset_index()\n",
    "\n",
    "    info_group_datas.drop('YearMonth', axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "    # info login -----------------------------------------------------------------\n",
    "\n",
    "    info_login.dropna(inplace = True)\n",
    "\n",
    "    info_login['c_time'] = pd.to_datetime(info_login['c_time'])\n",
    "    info_login['date'] = info_login.c_time.dt.date\n",
    "    info_login['YearMonth'] = info_login.c_time.dt.to_period('M')\n",
    "\n",
    "\n",
    "    login_group = info_login.groupby(['YearMonth','date'])\n",
    "    login_col = ['login_id']\n",
    "\n",
    "    login_group_data = pd.concat([\n",
    "        login_group[login_col].count().rename(columns = lambda s : 'loginsize.' + s)], axis = 1).reset_index()\n",
    "\n",
    "    login_group_data.drop('YearMonth', axis = 1, inplace = True)\n",
    "\n",
    "    from datetime import date, timedelta\n",
    "    info_compet['period_start'] = pd.to_datetime(info_compet['period_start'])\n",
    "    info_compet['merge_deadline'] = pd.to_datetime(info_compet['merge_deadline'])\n",
    "\n",
    "    dayframe = pd.DataFrame(columns = {'period_start'})\n",
    "    \n",
    "    # info compet -----------------------------------------------------------------\n",
    "    days = []\n",
    "    def daterange(d1, d2):\n",
    "        for i in range((d2 - d1).days + 1):\n",
    "            d3 = d1 + timedelta(days=i)\n",
    "            days.append(d3)\n",
    "\n",
    "    for i in range(0, len(info_compet)):\n",
    "        daterange(info_compet['period_start'][i], info_compet['merge_deadline'][i])\n",
    "\n",
    "    dayframe['period_start'] = days\n",
    "    result = pd.merge(dayframe, info_compet, how = 'outer')\n",
    "\n",
    "    info_compet.fillna(0, inplace = True)\n",
    "\n",
    "    info_compet['period_start'] = pd.to_datetime(info_compet['period_start'])\n",
    "    info_compet['date'] = info_compet.period_start.dt.date\n",
    "\n",
    "    traind = info_compet.groupby(['date'])\n",
    "    train_col = ['prize','max_file_per_day']\n",
    "    train_group_compet = pd.concat([\n",
    "        traind[train_col].sum().rename(columns = lambda s : 'dmean_' + s)\n",
    "    ], axis = 1).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "    # info user -----------------------------------------------------------------\n",
    "\n",
    "    info_user.dropna(inplace = True)\n",
    "\n",
    "    info_user['c_time'] = pd.to_datetime(info_user['c_time'])\n",
    "    info_user['date'] = info_user.c_time.dt.date\n",
    "    info_user['YearMonth'] = info_user.c_time.dt.to_period('M')\n",
    "\n",
    "    year_month = info_user.groupby(['YearMonth','date'])\n",
    "\n",
    "    col_a = ['id']\n",
    "    a_col = ['entered_competition_cnt', 'talk_board_cnt']\n",
    "\n",
    "    info_user_group = pd.concat([\n",
    "        year_month[col_a].count().rename(columns = lambda s : 'dcount_' + s),\n",
    "        year_month[a_col].sum().rename(columns = lambda s : 'dmean_' + s)\n",
    "    ],axis = 1).reset_index()\n",
    "\n",
    "    info_user_group.drop('YearMonth', axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "    # train -----------------------------------------------------------------\n",
    "\n",
    "    train['DateTime'] = pd.to_datetime(train['DateTime'])\n",
    "    train['date'] = train.DateTime.dt.date\n",
    "    train_final = train.groupby('date').sum().reset_index()\n",
    "\n",
    "    train['Year'] = train['DateTime'].dt.year\n",
    "    train['WeekDay'] = train['DateTime'].dt.weekday\n",
    "    train['Day'] = train['DateTime'].dt.day\n",
    "    train['YearMonth'] = train['DateTime'].dt.to_period('M')\n",
    "\n",
    "    traind = train.groupby(['YearMonth','date'])\n",
    "\n",
    "    train_col = ['Users', 'Sessions', 'NewVisitors', 'PageViews']\n",
    "\n",
    "    train_groups = pd.concat([\n",
    "        traind[train_col].mean().rename(columns = lambda s : 'dmean_' + s),\n",
    "        traind[train_col].sum().rename(columns = lambda s : 'dsum_' + s)\n",
    "    ], axis = 1).reset_index()\n",
    "\n",
    "\n",
    "    #train_md = pd.merge(train_groups, train_groupm, on ='YearMonth', how = 'left')\n",
    "    train_groups.drop('YearMonth', axis = 1, inplace = True)\n",
    "    train_groups\n",
    "\n",
    "    train = pd.merge(train_final, train_groups, on = 'date', how = 'left')\n",
    "    train_sub = pd.merge(train, info_group_datas, on = ['date'], how = 'left')\n",
    "    train_login = pd.merge(train_sub,login_group_data, on = ['date'], how ='left')\n",
    "    train_user = pd.merge(train_login, info_user_group, on = ['date'], how = 'left')\n",
    "    train = pd.merge(train_user, train_group_compet, on = ['date'], how = 'left')\n",
    "    train.fillna(0, inplace = True)\n",
    "    \n",
    "    return train\n",
    "\n",
    "train = pipline(info_sub, info_compet, info_user, train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini = train.iloc[:,1:].min()\n",
    "size = train.iloc[:,1:].max() - train.iloc[:,1:].min()\n",
    "train.iloc[:,1:] = (train.iloc[:,1:] -  mini) / size\n",
    "\n",
    "input_window = 30\n",
    "output_window = 7\n",
    "\n",
    "window_x = np.zeros((train.shape[0] - (input_window + output_window), input_window, 19))\n",
    "window_y = np.zeros((train.shape[0] - (input_window + output_window), output_window, 19))\n",
    "\n",
    "for start in range(train.shape[0] - (input_window + output_window)):\n",
    "    end = start + input_window    \n",
    "    window_x[start,:, :] = train.iloc[start : end                , 1: ].values\n",
    "    window_y[start,:, :] = train.iloc[end   : end + output_window, 1: ].values\n",
    "window_x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (size, timestep, feature) \n",
    "inputs = Input(shape=(None, 15))\n",
    "# print(inputs.shape)\n",
    "x = Conv1D(30, kernel_size=5, padding='causal')(inputs)\n",
    "x = LSTM(30, return_sequences=True)(x)\n",
    "x = LSTM(30, return_sequences=True)(x)\n",
    "# x = LSTM(30, return_sequences=True)(x)\n",
    "out_time = Dense(4)(x[:,-7:, :]) # Dense(추출하는 컬럼 수) x[:, -출력 윈도사이즈, :]\n",
    "out_time = K.reshape(out_time, (-1,7,4)) # -1, 아웃_윈도사이즈, 추출 컬럼 수\n",
    "model = Model(inputs=inputs, outputs=out_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_minus = ReduceLROnPlateau(monitor='val_loss', patience=40, verbose=1, min_lr=1e-7, factor=0.1) # lr_m 40  / es 80 / lr 2e-2 / 400 epochs\n",
    "ckpt_path = './LSTM.ckpt'\n",
    "check = ModelCheckpoint(ckpt_path,\n",
    "                       save_best_only=True,\n",
    "                       save_weights_only=True,\n",
    "                       monitor='val_loss',\n",
    "                       mode='min',\n",
    "                       verbose=1)\n",
    "es = EarlyStopping(patience=80, monitor='val_loss') # 60 -> 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(1e-2) ,loss= 'mse') # ADAM 2e-2 : 0.017 => 3.34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=window_x, y=window_y, epochs=400, verbose=0, callbacks=[check, es, lr_minus], validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(history.history['loss'], 'b', label = 'loss')\n",
    "ax.plot(history.history['val_loss'], 'r', label='val_loss')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('loss')\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_month = tf.convert_to_tensor(window_x[-1,:,:][np.newaxis,...])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "last_month = train.iloc[-30:,:].values[np.newaxis,...]\n",
    "last_month = tf.convert_to_tensor(last_month)\n",
    "\n",
    "\n",
    "for start in range((len(submission) - output_window)//7 + 2):\n",
    "    start = start * 7\n",
    "    next_week = model.predict(last_month)\n",
    "    last_month = tf.concat([last_month[:,7:,:], next_week], axis = 1)\n",
    "    \n",
    "    pred_week = next_week.reshape(output_window,4)\n",
    "    pred_week = pred_week * size.values + mini.values\n",
    "    pred_week = pred_week.astype(int)\n",
    "    \n",
    "    if start/7 == (len(submission) - output_window)//7 + 1:\n",
    "        submission.iloc[start :, 1:] = pred_week[-submission.iloc[start :, 1:].shape[0]:,:]\n",
    "    else:\n",
    "        submission.iloc[start : start + output_window, 1:] = pred_week\n",
    "submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
